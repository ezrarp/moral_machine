{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5762aeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseID</th>\n",
       "      <th>CrossingSignal</th>\n",
       "      <th>NumberOfCharacters</th>\n",
       "      <th>Saved</th>\n",
       "      <th>LeftHand</th>\n",
       "      <th>Man</th>\n",
       "      <th>Woman</th>\n",
       "      <th>Pregnant</th>\n",
       "      <th>Stroller</th>\n",
       "      <th>OldMan</th>\n",
       "      <th>...</th>\n",
       "      <th>LargeMan</th>\n",
       "      <th>Criminal</th>\n",
       "      <th>MaleExecutive</th>\n",
       "      <th>FemaleExecutive</th>\n",
       "      <th>FemaleAthlete</th>\n",
       "      <th>MaleAthlete</th>\n",
       "      <th>FemaleDoctor</th>\n",
       "      <th>MaleDoctor</th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FuBqcfGjNSZTGLmJL</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FuBqcfGjNSZTGLmJL</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FuBqbkH5834oodYJA</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FuBqbkH5834oodYJA</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FuBq4j5Kw9tYatn35</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ResponseID  CrossingSignal  NumberOfCharacters  Saved  LeftHand  \\\n",
       "0  FuBqcfGjNSZTGLmJL               2                 1.0      0       0.0   \n",
       "1  FuBqcfGjNSZTGLmJL               0                 1.0      1       1.0   \n",
       "2  FuBqbkH5834oodYJA               0                 5.0      1       0.0   \n",
       "3  FuBqbkH5834oodYJA               0                 1.0      0       1.0   \n",
       "4  FuBq4j5Kw9tYatn35               0                 2.0      1       1.0   \n",
       "\n",
       "   Man  Woman  Pregnant  Stroller  OldMan  ...  LargeMan  Criminal  \\\n",
       "0  0.0    0.0       0.0       0.0     0.0  ...       0.0       0.0   \n",
       "1  1.0    0.0       0.0       0.0     0.0  ...       0.0       0.0   \n",
       "2  0.0    2.0       0.0       1.0     0.0  ...       0.0       0.0   \n",
       "3  0.0    0.0       0.0       0.0     0.0  ...       0.0       0.0   \n",
       "4  0.0    1.0       0.0       0.0     0.0  ...       0.0       0.0   \n",
       "\n",
       "   MaleExecutive  FemaleExecutive  FemaleAthlete  MaleAthlete  FemaleDoctor  \\\n",
       "0            0.0              0.0            0.0          1.0           0.0   \n",
       "1            0.0              0.0            0.0          0.0           0.0   \n",
       "2            0.0              0.0            0.0          1.0           0.0   \n",
       "3            0.0              0.0            0.0          1.0           0.0   \n",
       "4            0.0              0.0            1.0          0.0           0.0   \n",
       "\n",
       "   MaleDoctor  Dog  Cat  \n",
       "0         0.0  0.0  0.0  \n",
       "1         0.0  0.0  0.0  \n",
       "2         1.0  0.0  0.0  \n",
       "3         0.0  0.0  0.0  \n",
       "4         0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = 'sample_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9691295c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6099113813743744,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.61      0.63      0.62    104934\\n           1       0.61      0.59      0.60    105067\\n\\n    accuracy                           0.61    210001\\n   macro avg       0.61      0.61      0.61    210001\\nweighted avg       0.61      0.61      0.61    210001\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = data.drop(columns=['Saved', 'ResponseID', 'CrossingSignal', 'LeftHand'])\n",
    "y = data['Saved']\n",
    "\n",
    "#standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59fd4d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.00011980961298880347,\n",
       " {'NumberOfCharacters': 0.12200854830201076,\n",
       "  'Man': 0.0449679393148702,\n",
       "  'Woman': 0.08122838402915801,\n",
       "  'Pregnant': 0.11848633466630495,\n",
       "  'Stroller': 0.12360055940410383,\n",
       "  'OldMan': -0.14069515253851492,\n",
       "  'OldWoman': -0.10548868262530585,\n",
       "  'Boy': 0.1686447164168631,\n",
       "  'Girl': 0.21158913086943062,\n",
       "  'Homeless': 0.04845537312403934,\n",
       "  'LargeWoman': 0.01165602405900479,\n",
       "  'LargeMan': -0.041827647656281455,\n",
       "  'Criminal': 0.03796677254430952,\n",
       "  'MaleExecutive': 0.030403802544820745,\n",
       "  'FemaleExecutive': 0.08662294685667438,\n",
       "  'FemaleAthlete': 0.10226548636110484,\n",
       "  'MaleAthlete': 0.06087666061800518,\n",
       "  'FemaleDoctor': 0.08621033870730765,\n",
       "  'MaleDoctor': 0.0506915745930437,\n",
       "  'Dog': -0.1495594885909117,\n",
       "  'Cat': -0.17057396480812248})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model equation\n",
    "intercept = model.intercept_[0]\n",
    "coefficients = model.coef_[0]\n",
    "feature_names = X.columns\n",
    "coefficients_dict = {feature: coef for feature, coef in zip(feature_names, coefficients)}\n",
    "\n",
    "intercept, coefficients_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9af0162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliferzincan/miniconda3/envs/cog260/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6949142857142857 0.6949142857142857 0.6948952380952381 0.6949142857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if len(data) % 2 != 0:\n",
    "    data = data.iloc[:-1]\n",
    "excluded_cols = ['Saved', 'CrossingSignal', 'LeftHand']\n",
    "feature_cols = data.select_dtypes(include=[np.number]).columns.drop(excluded_cols)\n",
    "diffs = []\n",
    "for i in range(0, len(data), 2):\n",
    "    diff = data.loc[i + 1, feature_cols] - data.loc[i, feature_cols]\n",
    "    diff['Saved'] = data.loc[i + 1]['Saved']\n",
    "    diffs.append(diff)\n",
    "\n",
    "diff_data = pd.DataFrame(diffs)\n",
    "    \n",
    "X = diff_data.drop('Saved', axis=1)\n",
    "y = diff_data['Saved']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "# L2 Regularization with a stronger regularization strength\n",
    "model_l2 = LogisticRegression(C=0.1)\n",
    "\n",
    "# L1 Regularization\n",
    "model_l1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "# Elastic-Net Regularization\n",
    "model_elasticnet = LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga')\n",
    "\n",
    "model_l2.fit(X_train, y_train)\n",
    "model_l1.fit(X_train, y_train)\n",
    "model_elasticnet.fit(X_train, y_train)\n",
    "\n",
    "#initial model\n",
    "y_pred = model2.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "#l2 reg model\n",
    "y_pred_l2 = model_l2.predict(X_test)\n",
    "accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
    "report_l2 = classification_report(y_test, y_pred_l2)\n",
    "\n",
    "#l1 reg model\n",
    "y_pred_l1 = model_l1.predict(X_test)\n",
    "accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n",
    "report_l1 = classification_report(y_test, y_pred_l1)\n",
    "\n",
    "#elastic net model\n",
    "y_pred_en = model_elasticnet.predict(X_test)\n",
    "accuracy_en = accuracy_score(y_test, y_pred_en)\n",
    "report_en = classification_report(y_test, y_pred_en)\n",
    "\n",
    "print(accuracy, accuracy_l2, accuracy_l1, accuracy_en)\n",
    "#print(report)\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "911dc4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.06988419004525663,\n",
       " {'NumberOfCharacters': 0.4774084844774991,\n",
       "  'Man': 0.1124558959061344,\n",
       "  'Woman': 0.17937784731510578,\n",
       "  'Pregnant': 0.09201380436125119,\n",
       "  'Stroller': 0.10801290627446322,\n",
       "  'OldMan': -0.09336398925081606,\n",
       "  'OldWoman': -0.055176155383339046,\n",
       "  'Boy': 0.24255254376484744,\n",
       "  'Girl': 0.28593989254411334,\n",
       "  'Homeless': -0.016682623758987204,\n",
       "  'LargeWoman': 0.07416481273508557,\n",
       "  'LargeMan': 0.009256527379388677,\n",
       "  'Criminal': -0.08776817647578641,\n",
       "  'MaleExecutive': 0.0376877130482707,\n",
       "  'FemaleExecutive': 0.07964666421190907,\n",
       "  'FemaleAthlete': 0.19648144616343244,\n",
       "  'MaleAthlete': 0.1432124666669523,\n",
       "  'FemaleDoctor': 0.107085662102036,\n",
       "  'MaleDoctor': 0.09160014512354905,\n",
       "  'Dog': -0.1796230520274297,\n",
       "  'Cat': -0.2087021290063989})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model equation\n",
    "intercept = model2.intercept_[0]\n",
    "coefficients = model2.coef_[0]\n",
    "feature_names = X.columns\n",
    "coefficients_dict = {feature: coef for feature, coef in zip(feature_names, coefficients)}\n",
    "\n",
    "intercept, coefficients_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
      "/var/folders/wy/h_0zk9hd5hd7ss0r3nrxbxvc0000gn/T/ipykernel_24223/2955267174.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:  0.6793619047619047\n",
      "Random Forest Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69     54169\n",
      "           1       0.67      0.65      0.66     50831\n",
      "\n",
      "    accuracy                           0.68    105000\n",
      "   macro avg       0.68      0.68      0.68    105000\n",
      "weighted avg       0.68      0.68      0.68    105000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "if len(data) % 2 != 0:\n",
    "    data = data.iloc[:-1]\n",
    "\n",
    "excluded_cols = ['Saved', 'CrossingSignal', 'LeftHand']\n",
    "feature_cols = data.select_dtypes(include=[np.number]).columns.drop(excluded_cols)\n",
    "\n",
    "diffs = []\n",
    "for i in range(0, len(data), 2):\n",
    "    diff = data.loc[i + 1, feature_cols] - data.loc[i, feature_cols]\n",
    "    diff['Saved'] = data.loc[i + 1]['Saved']\n",
    "    diffs.append(diff)\n",
    "\n",
    "diff_data = pd.DataFrame(diffs)\n",
    "\n",
    "# Compute interaction terms\n",
    "interaction_data = pd.DataFrame(index=diff_data.index)\n",
    "for comb in combinations(feature_cols, 2):\n",
    "    col_name = 'interaction_' + '_'.join(comb)\n",
    "    interaction_data[col_name] = diff_data[comb[0]] * diff_data[comb[1]]\n",
    "\n",
    "# Concatenate the interaction terms\n",
    "diff_data = pd.concat([diff_data, interaction_data], axis=1)\n",
    "\n",
    "X = diff_data.drop('Saved', axis=1)\n",
    "y = diff_data['Saved']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Accuracy: \", accuracy_rf)\n",
    "print(\"Random Forest Classification Report: \\n\", report_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time= 1.9min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time= 1.9min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time= 1.9min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time= 3.7min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time= 3.7min\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time= 1.8min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time= 3.8min\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time= 1.9min\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time= 1.8min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time= 5.6min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time= 5.6min\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time= 3.6min\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time= 3.6min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time= 1.8min\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time= 3.7min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time= 5.7min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time= 1.8min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time= 1.8min\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time= 5.6min\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time= 5.5min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time= 3.6min\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=300; total time= 5.5min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time= 3.6min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time= 3.7min\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=  45.0s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=  46.9s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=  47.1s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=  46.7s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=  46.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time= 2.2min\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time= 2.3min\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=  46.5s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time= 2.2min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time= 6.0min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time= 6.1min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=300; total time= 6.0min\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=  50.5s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=  51.1s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=  51.4s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time= 1.4min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time= 1.4min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time= 1.3min\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time= 2.5min\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time= 1.3min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time= 1.3min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time= 2.6min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time= 1.4min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time= 2.6min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time= 2.7min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time= 1.4min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time= 1.4min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time= 1.4min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time= 4.2min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time= 4.2min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time= 2.8min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time= 2.9min\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time= 4.1min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time= 2.8min\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time= 1.7min\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time= 1.7min\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time= 1.7min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time= 4.0min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time= 4.0min\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time= 1.8min\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time= 3.4min\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time= 3.3min\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time= 1.5min\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time= 1.5min\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time= 4.7min\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time= 4.8min\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time= 4.7min\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time= 3.0min\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time= 4.4min\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time= 4.2min\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=20.1min\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=21.1min\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=20.8min\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=23.7min\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=23.7min\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=21.2min\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=23.8min\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=21.5min\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time= 3.8min\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time= 3.3min\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time= 3.2min\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time= 2.7min\n",
      "Best Random Forest Accuracy:  0.6926190476190476\n",
      "Best Random Forest Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.72      0.71     54169\n",
      "           1       0.69      0.67      0.68     50831\n",
      "\n",
      "    accuracy                           0.69    105000\n",
      "   macro avg       0.69      0.69      0.69    105000\n",
      "weighted avg       0.69      0.69      0.69    105000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "\n",
    "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
    "report_best_rf = classification_report(y_test, y_pred_best_rf)\n",
    "\n",
    "print(\"Best Random Forest Accuracy: \", accuracy_best_rf)\n",
    "print(\"Best Random Forest Classification Report: \\n\", report_best_rf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
